# Project Structure

This document describes the directory structure and organization of the polymer property prediction project built on Chemprop v2.

## Directory Overview

### `data/`
Contains input datasets in CSV format for training and evaluation. Datasets typically include:
- SMILES strings for molecular structures
- Target property values (e.g., HOMO, LUMO, bandgap, optical properties)
- Optional dataset-specific descriptors
- Metadata columns (e.g., dataset source, polymer type)

### `scripts/`
Training, evaluation, and utility scripts for the project. See [`scripts/README.md`](scripts/README.md) for detailed documentation of each script.

**Subdirectories:**
- `python/` - Python scripts for training models, evaluation, and data processing
- `shell/` - Bash scripts for batch job submission and experiment generation (PBS/HPC)

### `chemprop/`
Modified Chemprop v2 library source code with custom extensions for polymer property prediction. This is a fork of the main Chemprop repository with project-specific modifications.

### `checkpoints/`
Saved model checkpoints organized by experiment configuration:
- Path pattern: `{dataset}__{target}{descriptors}{rdkit}{batch_norm}{size}__rep{split}/`
- Contains PyTorch Lightning checkpoint files (`.ckpt`)
- Checkpoints are saved only when `--save_checkpoint` flag is used
- Enables model resumption and inference on new data

### `preprocessing/`
Preprocessing metadata and artifacts for reproducible feature engineering:

**For graph models (DMPNN, AttentiveFP):**
- Location: `preprocessing/{ModelName}/{experiment_name}/`
- Files per split:
  - `preprocessing_metadata_split_{i}.json` - Full preprocessing configuration
  - `correlation_mask.npy` - Boolean mask for correlated feature removal
  - `constant_features_removed.npy` - Indices of constant features removed
  - `descriptor_scaler.pkl` - Fitted StandardScaler for descriptor normalization

**For tabular models (Linear, RF, XGB):**
- Location: `out/{target}/` (or with config suffix for different feature sets)
- Files per split:
  - `preprocessing_metadata_split_{i}.json` - Preprocessing configuration
  - `descriptor_imputer_{i}.pkl` - Fitted median imputer
  - `constant_mask_{i}.npy` - Boolean mask for constant removal
  - `corr_mask_{i}.npy` - Boolean mask for correlation filtering
  - `split_{i}.txt` - List of selected feature names
  - `feature_scaler_split_{i}_{Model}.pkl` - Per-model input scaler

### `out/`
Feature selection and preprocessing outputs for tabular models, organized by target property. Contains the preprocessing artifacts described above.

### `results/`
Model evaluation results organized by model type:

**Structure:**
- `results/{ModelName}/{dataset}{config}_results_{target}.csv`
- Each CSV contains metrics across all splits for a specific target
- Columns: `target`, `split`, `MAE`, `R2`, `RMSE` (for regression)

**Model directories:**
- `DMPNN/` - D-MPNN (Directed Message Passing Neural Network) results
- `AttentiveFP/` - AttentiveFP graph neural network results
- `tabular/` - Tabular baseline results (Linear, RF, XGB combined)

**Filename suffixes:**
- `__desc` - Dataset-specific descriptors included
- `__rdkit` - RDKit 2D descriptors included
- `__batch_norm` - Batch normalization enabled
- `__size{N}` - Training set subsampled to N samples (e.g., `__size500`)

### `predictions/`
Saved predictions (y_true, y_pred) for each experiment when `--save_predictions` flag is used. Organized by model and configuration for learning curve analysis and error analysis.

### `plots/`
Generated visualizations and figures:

**Subdirectories:**
- `multi_model/` - Multi-model comparison learning curves
  - Learning curves comparing all models across training set sizes
  - Separate plots per target and metric (MAE, R², RMSE)
  - `model_comparison_summary.csv` - Best model performance table
- Individual model learning curve plots (generated by `plot_opv_learning_curves.py`)

### `requirements/`
Python package dependencies organized by purpose:
- `requirements.txt` - Core dependencies
- `requirements-dev.txt` - Development tools
- `requirements-docs.txt` - Documentation generation
- Additional requirement files for specific environments

## Configuration Files

### `train_config.yaml`
Central configuration file for training experiments:
- **Global settings:** Random seed, replicates, epochs, patience
- **Dataset-specific descriptors:** Feature columns to use per dataset
- **Dataset ignore columns:** Columns to exclude from training
- **Model-specific settings:** SMILES column names, model-specific ignore columns
- **Path configuration:** Data, checkpoint, results, and output directories

### `experiments.yaml`
Batch experiment configuration for shell script generation (used by `generate_training_script.sh`)

### `evaluation_config.yaml`
Configuration for batch model evaluation scripts

## Key Python Scripts (Root Level)

### `plot_multi_model_learning_curves.py`
Generates comprehensive multi-model comparison plots:
- Loads results from all models (DMPNN, AttentiveFP, tabular baselines)
- Creates learning curves showing performance vs. training set size
- Produces comparison tables identifying best models per target/metric
- Handles multiple model variants (e.g., with/without descriptors)

### `plot_opv_learning_curves.py`
Generates learning curves for individual models or datasets:
- Plots performance metrics across different training sizes
- Supports filtering by model variant and target
- Creates publication-ready figures with error bars

### `make_wdmpnn_input.py`
Preprocesses SMILES strings for weighted D-MPNN (wDMPNN):
- Converts standard SMILES to wDMPNN input format
- Handles polymer-specific molecular representations
- Required for training wDMPNN models

## FiLM Conditioning (Early Descriptor Fusion)

The codebase supports three modes for integrating global descriptors (e.g., DoP, Molality, Density) into GNN models via the `--fusion_mode` argument:

| Mode | Behavior |
|------|----------|
| `none` | No descriptors used |
| `late_concat` | **(default)** Descriptors concatenated to graph embedding after message passing |
| `film` | FiLM early conditioning — descriptors modulate hidden states *inside* message passing |

### How FiLM Works

At each message passing layer *l*, a small MLP computes per-sample modulation parameters from the standardized descriptor vector **d** ∈ ℝ^D:

```
(γ_l, β_l) = MLP_l(d)        # shape [B, H] each
h_l = (1 + tanh(γ_l)) · h_l + β_l
```

The γ/β are broadcast over all nodes or directed edges belonging to each molecule in the batch using the `bmg.batch` graph-ID mapping.

**Architecture:** Shared trunk MLP (`Linear(D→film_hidden_dim), ReLU`) + per-layer linear heads (`Linear(film_hidden_dim→2H)`, split into γ and β). Heads are zero-initialized so the model starts as an unmodulated baseline.

### CLI Arguments

```
--fusion_mode {none,late_concat,film}   # default: late_concat
--film_layers {all,last}                # which MP layers to modulate (default: all)
--film_hidden_dim INT                   # FiLM MLP trunk hidden dim (default: MP hidden dim)
```

### Usage Examples

```bash
# Standard DMPNN with late descriptor concatenation (default)
python train_graph.py --dataset_name my_dataset --incl_desc --incl_rdkit

# DMPNN with FiLM early conditioning
python train_graph.py --dataset_name my_dataset --incl_desc --incl_rdkit --fusion_mode film

# FiLM only on last MP layer, custom hidden dim
python train_graph.py --dataset_name my_dataset --incl_desc --fusion_mode film --film_layers last --film_hidden_dim 128

# GIN with FiLM
python train_graph.py --dataset_name my_dataset --model_name GIN --incl_desc --fusion_mode film
```

### Supported Models

FiLM conditioning works with: **DMPNN**, **wDMPNN**, **PPG**, **DMPNN_SumPool**, **DMPNN_AttnPool**, **GIN**, **GIN0**, **GINE**, **GAT**, **GATv2**. It is not supported for **DMPNN_DiffPool** (falls back to `late_concat` with a warning).

### Smoke Test

```bash
python tests/test_film_smoke.py
```

Runs forward passes with `fusion_mode=film` on all supported model types and verifies output shapes, backward compatibility, and gradient flow.

## Auxiliary Descriptor Prediction (Multi-Task Training)

The codebase supports an auxiliary training mode where the GNN simultaneously predicts the main property **y** and selected polymer descriptors (e.g., DoP, Density) as auxiliary regression targets. **Crucially:** in this mode, descriptors are NOT fed as model inputs — they are only prediction targets.

### How It Works

The graph embedding **Z** (output of message passing + aggregation) feeds into two heads:

```
Z ─┬─→ [Main FFN]  → y_hat   (main prediction, shape [B, T_main])
   └─→ [Aux MLP]   → d_hat   (descriptor prediction, shape [B, T_aux])
```

The combined training loss is:

```
L = L_main(y_hat, y) + λ_aux · L_aux(d_hat, d_target)
```

- **L_main**: Chemprop's standard loss (MSE/MAE)
- **L_aux**: MSE on standardized auxiliary targets (per-split train-set mean/std)
- **λ_aux**: Configurable weight (default 0.1)

The auxiliary head is a small MLP: `Linear(H → H/2) → ReLU → Linear(H/2 → T_aux)`.

### CLI Arguments

```
--aux_task {off,predict_descriptors}     # default: off
--aux_descriptor_cols "DoP,Density"      # comma-separated column names from CSV
--lambda_aux FLOAT                       # auxiliary loss weight (default: 0.1)
```

### Constraints

- `--aux_task=predict_descriptors` is **incompatible** with `--incl_desc`, `--incl_rdkit`, and `--fusion_mode=film`
- Descriptors must NOT be model inputs when used as auxiliary targets

### Usage Examples

```bash
# DMPNN with auxiliary descriptor prediction
python train_graph.py --dataset_name htpmd --model_name DMPNN \
    --aux_task predict_descriptors --aux_descriptor_cols "DoP,Density" --lambda_aux 0.1

# GIN with auxiliary task and custom lambda
python train_graph.py --dataset_name htpmd --model_name GIN \
    --aux_task predict_descriptors --aux_descriptor_cols "DoP,Density,Molality" --lambda_aux 0.05
```

### Smoke Test

```bash
python tests/test_aux_task_smoke.py
```

Runs 6 tests verifying aux head output shape, combined loss computation, gradient flow through both heads, target splitting, NaN handling, and backward compatibility.

## Workflow Overview

1. **Data Preparation:** Place datasets in `data/` directory
2. **Configuration:** Set up `train_config.yaml` with dataset-specific settings
3. **Training:** Run training scripts from `scripts/python/` or submit batch jobs via `scripts/shell/`
4. **Preprocessing:** Artifacts automatically saved to `preprocessing/` or `out/`
5. **Checkpoints:** Model checkpoints saved to `checkpoints/` (if enabled)
6. **Results:** Evaluation metrics saved to `results/{ModelName}/`
7. **Visualization:** Generate plots using `plot_multi_model_learning_curves.py`

## Notes

- All paths support train_size suffixes (`__size{N}`) to prevent conflicts between experiments with different training set sizes
- Preprocessing is cached and reused when compatible (same features, splits, and configuration)
- Cross-validation and holdout validation are automatically selected based on dataset size
- The project uses PBS job submission scripts for HPC environments but can be adapted for other schedulers
