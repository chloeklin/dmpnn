# Project Structure

This document describes the directory structure and organization of the polymer property prediction project built on Chemprop v2.

## Directory Overview

### `data/`
Contains input datasets in CSV format for training and evaluation. Datasets typically include:
- SMILES strings for molecular structures
- Target property values (e.g., HOMO, LUMO, bandgap, optical properties)
- Optional dataset-specific descriptors
- Metadata columns (e.g., dataset source, polymer type)

### `scripts/`
Training, evaluation, and utility scripts for the project. See [`scripts/README.md`](scripts/README.md) for detailed documentation of each script.

**Subdirectories:**
- `python/` - Python scripts for training models, evaluation, and data processing
- `shell/` - Bash scripts for batch job submission and experiment generation (PBS/HPC)

### `chemprop/`
Modified Chemprop v2 library source code with custom extensions for polymer property prediction. This is a fork of the main Chemprop repository with project-specific modifications.

### `checkpoints/`
Saved model checkpoints organized by experiment configuration:
- Path pattern: `{dataset}__{target}{descriptors}{rdkit}{batch_norm}{size}__rep{split}/`
- Contains PyTorch Lightning checkpoint files (`.ckpt`)
- Checkpoints are saved only when `--save_checkpoint` flag is used
- Enables model resumption and inference on new data

### `preprocessing/`
Preprocessing metadata and artifacts for reproducible feature engineering:

**For graph models (DMPNN, AttentiveFP):**
- Location: `preprocessing/{ModelName}/{experiment_name}/`
- Files per split:
  - `preprocessing_metadata_split_{i}.json` - Full preprocessing configuration
  - `correlation_mask.npy` - Boolean mask for correlated feature removal
  - `constant_features_removed.npy` - Indices of constant features removed
  - `descriptor_scaler.pkl` - Fitted StandardScaler for descriptor normalization

**For tabular models (Linear, RF, XGB):**
- Location: `out/{target}/` (or with config suffix for different feature sets)
- Files per split:
  - `preprocessing_metadata_split_{i}.json` - Preprocessing configuration
  - `descriptor_imputer_{i}.pkl` - Fitted median imputer
  - `constant_mask_{i}.npy` - Boolean mask for constant removal
  - `corr_mask_{i}.npy` - Boolean mask for correlation filtering
  - `split_{i}.txt` - List of selected feature names
  - `feature_scaler_split_{i}_{Model}.pkl` - Per-model input scaler

### `out/`
Feature selection and preprocessing outputs for tabular models, organized by target property. Contains the preprocessing artifacts described above.

### `results/`
Model evaluation results organized by model type:

**Structure:**
- `results/{ModelName}/{dataset}{config}_results_{target}.csv`
- Each CSV contains metrics across all splits for a specific target
- Columns: `target`, `split`, `MAE`, `R2`, `RMSE` (for regression)

**Model directories:**
- `DMPNN/` - D-MPNN (Directed Message Passing Neural Network) results
- `AttentiveFP/` - AttentiveFP graph neural network results
- `tabular/` - Tabular baseline results (Linear, RF, XGB combined)

**Filename suffixes:**
- `__desc` - Dataset-specific descriptors included
- `__rdkit` - RDKit 2D descriptors included
- `__batch_norm` - Batch normalization enabled
- `__size{N}` - Training set subsampled to N samples (e.g., `__size500`)

### `predictions/`
Saved predictions (y_true, y_pred) for each experiment when `--save_predictions` flag is used. Organized by model and configuration for learning curve analysis and error analysis.

### `plots/`
Generated visualizations and figures:

**Subdirectories:**
- `multi_model/` - Multi-model comparison learning curves
  - Learning curves comparing all models across training set sizes
  - Separate plots per target and metric (MAE, RÂ², RMSE)
  - `model_comparison_summary.csv` - Best model performance table
- Individual model learning curve plots (generated by `plot_opv_learning_curves.py`)

### `requirements/`
Python package dependencies organized by purpose:
- `requirements.txt` - Core dependencies
- `requirements-dev.txt` - Development tools
- `requirements-docs.txt` - Documentation generation
- Additional requirement files for specific environments

## Configuration Files

### `train_config.yaml`
Central configuration file for training experiments:
- **Global settings:** Random seed, replicates, epochs, patience
- **Dataset-specific descriptors:** Feature columns to use per dataset
- **Dataset ignore columns:** Columns to exclude from training
- **Model-specific settings:** SMILES column names, model-specific ignore columns
- **Path configuration:** Data, checkpoint, results, and output directories

### `experiments.yaml`
Batch experiment configuration for shell script generation (used by `generate_training_script.sh`)

### `evaluation_config.yaml`
Configuration for batch model evaluation scripts

## Key Python Scripts (Root Level)

### `plot_multi_model_learning_curves.py`
Generates comprehensive multi-model comparison plots:
- Loads results from all models (DMPNN, AttentiveFP, tabular baselines)
- Creates learning curves showing performance vs. training set size
- Produces comparison tables identifying best models per target/metric
- Handles multiple model variants (e.g., with/without descriptors)

### `plot_opv_learning_curves.py`
Generates learning curves for individual models or datasets:
- Plots performance metrics across different training sizes
- Supports filtering by model variant and target
- Creates publication-ready figures with error bars

### `make_wdmpnn_input.py`
Preprocesses SMILES strings for weighted D-MPNN (wDMPNN):
- Converts standard SMILES to wDMPNN input format
- Handles polymer-specific molecular representations
- Required for training wDMPNN models

## Workflow Overview

1. **Data Preparation:** Place datasets in `data/` directory
2. **Configuration:** Set up `train_config.yaml` with dataset-specific settings
3. **Training:** Run training scripts from `scripts/python/` or submit batch jobs via `scripts/shell/`
4. **Preprocessing:** Artifacts automatically saved to `preprocessing/` or `out/`
5. **Checkpoints:** Model checkpoints saved to `checkpoints/` (if enabled)
6. **Results:** Evaluation metrics saved to `results/{ModelName}/`
7. **Visualization:** Generate plots using `plot_multi_model_learning_curves.py`

## Notes

- All paths support train_size suffixes (`__size{N}`) to prevent conflicts between experiments with different training set sizes
- Preprocessing is cached and reused when compatible (same features, splits, and configuration)
- Cross-validation and holdout validation are automatically selected based on dataset size
- The project uses PBS job submission scripts for HPC environments but can be adapted for other schedulers
